from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, load_index_from_storage, StorageContext
from llama_index.core.vector_stores import SimpleVectorStore
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.indices.multi_modal import MultiModalVectorStoreIndex
from typing import List, Union, Optional
from llama_index.core.schema import ImageDocument, BaseNode, ImageNode
import os
from PIL import Image
from vl_embedding import VL_Embedding
import io
import hashlib
import warnings
import pickle
# å‡è®¾æ‰€æœ‰PPTé¡µé¢ä¸ºjpgæ ¼å¼å­˜åœ¨æŸä¸ªæ–‡ä»¶å¤¹ä¸‹

# class VL_EmbeddingAdapter(BaseEmbedding):
#     def __init__(self, vl_embed: VL_Embedding):
#         self.vl_embed = vl_embed

#     def embed(self, nodes: List[BaseNode]) -> List[List[float]]:
#         """Embed document nodes."""
#         # ç›´æ¥è°ƒç”¨ä½ å®šä¹‰çš„ __call__
#         nodes = self.vl_embed(nodes)
#         return [node.embedding for node in nodes]

#     def embed_query(self, query: str) -> List[float]:
#         """Embed a query string."""
#         return self.vl_embed.embed_text(query)[0]
def _hash_path(path: str) -> str:
    return hashlib.md5(path.encode("utf-8")).hexdigest()

def load_ppt_images_as_documents(folder_path: str, cache_path="./cache/image_doc_cache.pkl"):
    docs = []
    cache = {}

    # å¦‚æœå­˜åœ¨ç¼“å­˜ï¼Œå…ˆåŠ è½½
    if os.path.exists(cache_path):
        with open(cache_path, "rb") as f:
            cache = pickle.load(f)

    updated = False
    for fname in os.listdir(folder_path):
        if fname.endswith(".png") or fname.endswith(".jpg"):
            fpath = os.path.join(folder_path, fname)
            key = _hash_path(fpath)

            if key in cache:
                docs.append(cache[key])
            else:
                image = Image.open(fpath)
                buffer = io.BytesIO()
                image.save(buffer, format="PNG")
                img_bytes = buffer.getvalue()
                doc = ImageDocument(image=img_bytes, metadata={"file_path": fpath})
                docs.append(doc)
                cache[key] = doc
                updated = True

    # å†™å›ç¼“å­˜ï¼ˆåªåœ¨æœ‰æ–°æ–‡ä»¶æ—¶ï¼‰
    if updated:
        with open(cache_path, "wb") as f:
            pickle.dump(cache, f)

    return docs

class RobustMultiModalIndexer:
    def __init__(self, embed_model, strict_validation=False):
        self.embed_model = embed_model
        self.strict_validation = strict_validation
        
    def _create_hybrid_nodes(self, docs: List[ImageDocument]) -> List[ImageNode]:
        """å°†ImageDocumentè½¬æ¢ä¸ºåŒ…å«åŒé‡éªŒè¯çš„ImageNode"""
        nodes = []
        for doc in docs:
            try:
                # å¼ºåˆ¶ç±»å‹è½¬æ¢ç¡®ä¿èŠ‚ç‚¹ç»“æ„åˆè§„
                node = ImageNode(
                    image=doc.image,
                    text=doc.text or "",  # ç¡®ä¿ä¸ä¸ºNone
                    metadata=doc.metadata,
                    excluded_embed_metadata_keys=["file_path"],  # é˜²æ­¢å…ƒæ•°æ®å¹²æ‰°
                    excluded_llm_metadata_keys=["file_path"]
                )
                
                # äººå·¥éªŒè¯èŠ‚ç‚¹å†…å®¹
                if not node.image and self.strict_validation:
                    raise ValueError("Empty image content")
                    
                nodes.append(node)
            except Exception as e:
                warnings.warn(f"Skipped invalid document {doc.metadata.get('file_path','')}: {str(e)}")
        return nodes

    def build_index(self, docs: List[ImageDocument], persist_dir: str, rebuild:bool = False) -> MultiModalVectorStoreIndex:
        """ç»ˆæå¥å£®çš„ç´¢å¼•æ„å»ºæ–¹æ³•"""
        # è½¬æ¢èŠ‚ç‚¹ç±»å‹å¹¶è¿‡æ»¤æ— æ•ˆæ–‡æ¡£
        nodes = self._create_hybrid_nodes(docs)

         # ç¼“å­˜åŠ è½½é€»è¾‘
        if not rebuild and os.path.exists(persist_dir):
            print("ğŸ” å°è¯•åŠ è½½ç¼“å­˜ç´¢å¼•...")
            try:
                storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
                cached_index = MultiModalVectorStoreIndex.from_vector_store(
                    storage_context.vector_store,
                    embed_model=self.embed_model,
                    _validate_nodes=False  # ç¦ç”¨åŠ è½½æ—¶çš„éªŒè¯
                )
                print(f"âœ… åŠ è½½ç¼“å­˜ç´¢å¼•æˆåŠŸ (åŒ…å« {len(cached_index._index_struct.nodes)} ä¸ªèŠ‚ç‚¹)")
                return cached_index
            except Exception as e:
                print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}. å°†é‡å»ºç´¢å¼•...")

        # æ–°å»ºç´¢å¼•
        print("âš™ï¸ æ„å»ºæ–°ç´¢å¼•...")
        storage_context = StorageContext.from_defaults(
            vector_store=SimpleVectorStore()
        )
        
        index = MultiModalVectorStoreIndex(
            nodes=nodes,
            storage_context=storage_context,
            image_embed_model=self.embed_model,
            image_field="image",
            text_field="text",
            is_image_to_text=False,
            show_progress=True,
            _validate_nodes=False
        )
        
        # æŒä¹…åŒ–æ—¶å¼ºåˆ¶å†™å…¥
        index.storage_context.persist(
            persist_dir=persist_dir,# è¦†ç›–ç°æœ‰ç¼“å­˜
        )
        # print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ (åŒ…å« {len(index._index_struct.nodes)} ä¸ªèŠ‚ç‚¹)")
        return index
# # 4. æ£€ç´¢æœ€ç›¸å…³çš„é¡µé¢ï¼ˆå›¾ç‰‡ï¼‰ç»™å®šæ–‡æœ¬ query
def retrieve_topk(index, query, top_k=3):
    retriever = index.as_retriever(similarity_top_k=top_k)
    retrieved_nodes = retriever.retrieve(query)
    return retrieved_nodes

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    os.environ["OPENAI_API_KEY"] = "your_openai_api_key_here"
    # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    embed_model = VL_Embedding(
        model="vidore/colpali-v1.2",
        device="cuda:0",
        mode="image" 
    )
    query = "What are the economic implications of transitioning to a green economy in Southeast Asia?"
    # 2. åˆ›å»ºç´¢å¼•æ„å»ºå™¨
    index_builder =RobustMultiModalIndexer(embed_model)
    
    # 3. åŠ è½½å›¾åƒæ–‡æ¡£
    image_docs = load_ppt_images_as_documents("/data2/home/yankai/ppt_crawler/data/trend/trend_images/bain_report_southeast_asias_green_economy_2025")
    # image_docs = index_builder._create_hybrid_nodes(image_docs)  # ç¡®ä¿è½¬æ¢ä¸ºImageNode
    
    # 4. æ„å»ºç´¢å¼•
    index = index_builder.build_index(image_docs, persist_dir="./cache/indexes/bain_report_index")
    top_img = retrieve_topk(index, query, top_k=10)
    print(f"Top {len(top_img)} images retrieved for query '{query}':")






# # 5. æäº¤ç»™ LLM ç”Ÿæˆç­”æ¡ˆ
# def generate_answer(query, context_images, gpt_llm):
#     """
#     query: ç”¨æˆ·é—®é¢˜
#     context_images: ç»è¿‡æ£€ç´¢çš„ ImageDocument èŠ‚ç‚¹
#     gpt_llm: OpenAI GPT wrapper or LLM from llama_index
#     """
#     # ä½ å¯ä»¥é€‰æ‹©é™„å¸¦ context metadataï¼ˆæ¯”å¦‚æ–‡ä»¶åï¼‰ç»™æ¨¡å‹
#     context_str = "\n\n".join([f"Image page: {node.metadata.get('file_path', '')}" for node in context_images])
#     prompt = f"""
# You are answering a question based on visual slides (PPT pages). These are the most relevant slides:

# {context_str}

# Question: {query}

# Give a concise but informative answer.
# """
#     return gpt_llm.complete(prompt).text


# # 6. æ•´åˆç¤ºä¾‹
# if __name__ == "__main__":
#     folder = "./data/trend/trend_images/bain_report_southeast_asias_green_economy_2025"
#     query = "What strategies are proposed to decarbonize Southeast Asiaâ€™s economy?"

#     docs = load_ppt_images_as_documents(folder)
#     index = build_vector_index(docs, embed_model)

#     top_images = retrieve_topk(index, query, top_k=3)

#     # ä½ éœ€è¦åœ¨è¿™é‡Œé…ç½® GPT LLMï¼Œä¾‹å¦‚ï¼š
#     # from llama_index.llms import OpenAI
#     # gpt_llm = OpenAI(model="gpt-4", api_key="...")
    
#     # answer = generate_answer(query, top_images, gpt_llm)
#     # print("Answer:", answer)